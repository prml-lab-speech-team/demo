<html lang="en">
<head>
<meta charset="UTF-8">
<title>PRML Speech Team Demo Page</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.2/css/bootstrap.min.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.2/css/bootstrap-theme.min.css">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.2/js/bootstrap.min.js"></script>
  <style>
    .example {
    font-size:20px;
    padding-left:10px;
    }

    td{
        width:400px;
        text-align:center;
        white-space:nowrap;
        padding-left:15px;
    }
</style>
</head>

<body style='padding-left: 20px'>
  
<div class="container">

	<header role="banner">
	<h1 class="site-title">PRML Lab. Speech team</h1>		
	</header>
  PRML Lab. in Korea University (Director: Prof. Seong-Whan Lee)
	<br>
	<br>

	<main role="main">
    <article>
			<h3><a href="/Fre-Painter/">Fre-Painter: Audio Frequency Inpainting with Robust Speech Representation Learning of Masked Autoencoder</a></h3>
			<span>ICASSP (submitted), <time>2023</time></span>
		</article>
    <article>
			<h3><a href="/DiffProsody/">DiffProsody: Latent Diffusion-based Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training</a></h3>
			<span>ICASSP (submitted), <time>2023</time></span>
		</article>
    <article>
			<h3><a href="https://sh-lee-prml.github.io/hierspeech-demo/">HierSpeech: Bridging the Gap between Text and Speech by Hierarchical Variational Inference using Self-supervised Representation for Speech Synthesis</a></h3>
			<span>NeurIPS, <time>November 28, 2022</time></span>
		</article>
    <article>
			<h3><a href="/hrnoh/DCVC/">Duration Controllable Voice Conversion via Phoneme-Based Information Bottleneck</a></h3>
			<span>TASLP, <time>March 7, 2022</time></span>
		</article>

    <article>
			<h3><a href="/insun-hwang/StyleVC/">StyleVC: Non-parallel Voice Conversion with Adversarial Style Generalization</a></h3>
			<span>ICPR <time>2021</time></span>
		</article>
    <article>
			<h3><a href="/EmoQ-TTS/">EmoQ-TTS: Emotion intensity Quantization for Fine-grained Controllable Emotional Text-to-Speech</a></h3>
			<span>ICASSP, <time>May 23, 2022</time></span>
		</article>
    <article>
			<h3><a href="/FreGAN2/">Fre-GAN 2: Fast and Efficient Frequency-consistent Audio Synthesis</a></h3>
			<span>ICASSP, <time>May 23, 2022</time></span>
		</article>
    <article>
			<h3><a href="/PVAE-TTS/">PVAE-TTS: High-Quality Adaptive Text-to-Speech via Progressive Variational Autoencoder</a></h3>
			<span>ICASSP, <time>May 23, 2022</time></span>
		</article>
    <article>
			<h3><a href="/insun-hwang/RCVC/">RCVC: Rhythm Controllable Voice Conversion with Dynamic Information Bottleneck</a></h3>
			<span><time>2021</time></span>
		</article>
    <article>
			<h3><a href="https://anonymous-speech.github.io/voicemixer/index.html">VoiceMixer: Adversarial Voice Style Mixup</a></h3>
			<span>NeurIPS, <time>December 6, 2021</time></span>
		</article>
    <article>
			<h3><a href="/GC-TTS/">GC-TTS: Few-shot Speaker Adaptation with Geometric Constraints</a></h3>
			<span>SMC,<time>October 17, 2021</time></span>
		</article>
    <article>
			<h3><a href="/Reinforce-Aligner/">Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech</a></h3>
			<span>Interspeech, <time>August 30, 2021</time></span>
		</article>
    <article>
			<h3><a href="/FreGAN/">Fre-GAN: Adversarial Frequency-consistent Audio Synthesis</a></h3>
			<span>Interspeech, <time>August 30, 2021</time></span>
		</article>
    <article>
			<h3><a href="https://anonymsg.github.io/MSG/Demo/index.html">Multi-SpectroGAN: High-Diversity and High-Fidelity Spectrogram Generation with Adversarial Style Recombination for Speech Synthesis</a></h3>
			<span>AAAI2021, <time>May 18, 2021</time></span>
		</article>
    <article>
			<h3><a href="/jhkim-few">Few-shot Speaker Adaptation with Geometric Constraints</a></h3>
			<span><time>2020</time></span>
		</article>
    <article>
			<h3><a href="/chsong/">Individual Prosody Control using Generated Duration and Information via Mel Spectrogram Deformation</a></h3>
			<span><time>2019</time></span>
		</article>
		<article>
			<h3><a href="/sh_lee/SpectroGAN.html/">SpectroGAN: Spectrogram Generation with Adversarial Networks for Speech Synthesis</a></h3>
			<span><time>2019</time></span>
		</article>
  </main>
</div>

</body>
</html>